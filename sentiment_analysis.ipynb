{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.5.1-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: spacy\n",
      "Successfully installed spacy-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\rahul\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import string\n",
    "punc = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "from datetime import datetime\n",
    "\n",
    "! pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "webscrapped_data = pd.read_csv('C:\\\\Users\\\\rahul\\\\OneDrive\\\\Desktop\\\\Education\\\\Year 3\\\\Winter Sem\\\\ECO481\\\\Final Project\\\\webscrapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nov.26,2001'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webscrapped_data[\"date\"][0].split(\" \")[0] + webscrapped_data[\"date\"][0].split(\" \")[1] + webscrapped_data[\"date\"][0].split(\" \")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscrapped_data[\"date\"] = webscrapped_data[\"date\"].apply(lambda x: x.strip())\n",
    "webscrapped_data[\"date\"] = webscrapped_data[\"date\"].apply(lambda x: x.split(\" \")[0] + \" \" + x.split(\" \")[1] + \" \" +x.split(\" \")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday = webscrapped_data.groupby(\"date\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>paragraph_1</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China Sets Up Unified System\\nFor Central Secu...</td>\n",
       "      <td>April 1, 2001</td>\n",
       "      <td>SHANGHAI, China -- China has set up a unified ...</td>\n",
       "      <td>https://www.wsj.com/articles/SB986114850336013671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Market Advances\\nAs Bargain-Hunters\\nLift Fall...</td>\n",
       "      <td>April 1, 2001</td>\n",
       "      <td>Stocks rose Friday in choppy trading as invest...</td>\n",
       "      <td>https://www.wsj.com/articles/SB985954955766668512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ameritrade Cuts Work Force by 7%;\\nDiscloses I...</td>\n",
       "      <td>April 1, 2001</td>\n",
       "      <td>Ameritrade Holding</td>\n",
       "      <td>https://www.wsj.com/articles/SB985989186239735853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Darts Beat Out Reader Picks\\nAs Old Economy St...</td>\n",
       "      <td>April 1, 2001</td>\n",
       "      <td>And here you thought</td>\n",
       "      <td>https://www.wsj.com/articles/SB985730604488524893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Citigroup Online-Brokerage Site\\nOffers Servic...</td>\n",
       "      <td>April 1, 2001</td>\n",
       "      <td>Many banks now offer online brokerage services...</td>\n",
       "      <td>https://www.wsj.com/articles/SB985731019140575710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38509</th>\n",
       "      <td>The Summer of Our Discontent</td>\n",
       "      <td>September 4, 2011</td>\n",
       "      <td>This summer offered little respite for nervous...</td>\n",
       "      <td>https://www.wsj.com/articles/SB100014240531119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38510</th>\n",
       "      <td>Labor Day Blues</td>\n",
       "      <td>September 4, 2011</td>\n",
       "      <td>Two out of three unemployed Americans want the...</td>\n",
       "      <td>https://www.wsj.com/articles/SB100014240531119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38511</th>\n",
       "      <td>South Korea’s Hanjin Shipping Files for U.S. B...</td>\n",
       "      <td>September 4, 2016</td>\n",
       "      <td>South Korea’s</td>\n",
       "      <td>https://www.wsj.com/articles/south-koreas-hanj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38512</th>\n",
       "      <td>How to Detect Disaster-Relief Scams</td>\n",
       "      <td>September 4, 2017</td>\n",
       "      <td>Thomas Blaney</td>\n",
       "      <td>https://www.wsj.com/articles/how-to-detect-dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38513</th>\n",
       "      <td>Trump Lawyers Oppose DOJ’s Bid to Resume Revie...</td>\n",
       "      <td>September 9, 2022</td>\n",
       "      <td>WASHINGTON—Lawyers for former President</td>\n",
       "      <td>https://www.wsj.com/articles/trump-lawyers-opp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38514 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline               date  \\\n",
       "0      China Sets Up Unified System\\nFor Central Secu...      April 1, 2001   \n",
       "1      Market Advances\\nAs Bargain-Hunters\\nLift Fall...      April 1, 2001   \n",
       "2      Ameritrade Cuts Work Force by 7%;\\nDiscloses I...      April 1, 2001   \n",
       "3      Darts Beat Out Reader Picks\\nAs Old Economy St...      April 1, 2001   \n",
       "4      Citigroup Online-Brokerage Site\\nOffers Servic...      April 1, 2001   \n",
       "...                                                  ...                ...   \n",
       "38509                       The Summer of Our Discontent  September 4, 2011   \n",
       "38510                                    Labor Day Blues  September 4, 2011   \n",
       "38511  South Korea’s Hanjin Shipping Files for U.S. B...  September 4, 2016   \n",
       "38512                How to Detect Disaster-Relief Scams  September 4, 2017   \n",
       "38513  Trump Lawyers Oppose DOJ’s Bid to Resume Revie...  September 9, 2022   \n",
       "\n",
       "                                             paragraph_1  \\\n",
       "0      SHANGHAI, China -- China has set up a unified ...   \n",
       "1      Stocks rose Friday in choppy trading as invest...   \n",
       "2                                     Ameritrade Holding   \n",
       "3                                  And here you thought    \n",
       "4      Many banks now offer online brokerage services...   \n",
       "...                                                  ...   \n",
       "38509  This summer offered little respite for nervous...   \n",
       "38510  Two out of three unemployed Americans want the...   \n",
       "38511                                     South Korea’s    \n",
       "38512                                      Thomas Blaney   \n",
       "38513           WASHINGTON—Lawyers for former President    \n",
       "\n",
       "                                                    link  \n",
       "0      https://www.wsj.com/articles/SB986114850336013671  \n",
       "1      https://www.wsj.com/articles/SB985954955766668512  \n",
       "2      https://www.wsj.com/articles/SB985989186239735853  \n",
       "3      https://www.wsj.com/articles/SB985730604488524893  \n",
       "4      https://www.wsj.com/articles/SB985731019140575710  \n",
       "...                                                  ...  \n",
       "38509  https://www.wsj.com/articles/SB100014240531119...  \n",
       "38510  https://www.wsj.com/articles/SB100014240531119...  \n",
       "38511  https://www.wsj.com/articles/south-koreas-hanj...  \n",
       "38512  https://www.wsj.com/articles/how-to-detect-dis...  \n",
       "38513  https://www.wsj.com/articles/trump-lawyers-opp...  \n",
       "\n",
       "[38514 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_articlesday.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_44900\\1161128878.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top5_articlesday[\"link\"] = top5_articlesday[\"link\"].apply(lambda x: x.split(\"/\")[-1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top5_articlesday[\"link\"] = top5_articlesday[\"link\"].apply(lambda x: x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.read_csv(\"C:\\\\Users\\\\rahul\\\\OneDrive\\\\Desktop\\\\Education\\\\Year 3\\\\Winter Sem\\\\ECO481\\\\ourfirstscraper\\\\wsj_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.drop([\"article_names\", \"date\"], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[\"links\"] = topics[\"links\"].apply(lambda x: x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday = pd.merge(top5_articlesday, topics, left_on = \"link\", right_on = \"links\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday.drop([\"links\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday.drop([\"link\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(doc):\n",
    "    if type(doc) == str:\n",
    "        html_tag_free=doc.replace('<br />',' ')\n",
    "        stop_free = \" \".join([i for i in html_tag_free.lower().split() if i not in stop])\n",
    "        numb_free = \"\".join([i for i in stop_free if not i.isdigit()])\n",
    "        punc_free = \"\".join(ch for ch in numb_free if ch not in punc)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        stop_free = \" \".join([i for i in normalized.lower().split() if i not in stop])\n",
    "        one_letter_free = \" \".join( [w for w in stop_free.split() if len(w)>1] )\n",
    "    # dropping one letter words\n",
    "        return one_letter_free\n",
    "    else:\n",
    "        return doc\n",
    "\n",
    "top5_articlesday[\"cleaned_headline\"] = top5_articlesday[\"headline\"].apply(lambda x: clean(x))\n",
    "top5_articlesday[\"cleaned_paragraph\"] = top5_articlesday[\"paragraph_1\"].apply(lambda x: clean(x))\n",
    "top5_articlesday[\"cleaned_topic\"] = top5_articlesday[\"topics\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday[\"date\"] = pd.to_datetime(top5_articlesday[\"date\"], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "top5_articlesday[\"headline_sentiment\"] = top5_articlesday[\"cleaned_headline\"].apply(lambda x: obj.polarity_scores(x)['compound'])\n",
    "top5_articlesday[\"paragraph_sentiment\"] = top5_articlesday[\"cleaned_paragraph\"].apply(lambda x: obj.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38514.000000\n",
       "mean        -0.010931\n",
       "std          0.311756\n",
       "min         -0.944200\n",
       "25%         -0.102700\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          0.910000\n",
       "Name: headline_sentiment, dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found sentiment scores for both headlines and paragraphs\n",
    "# Need the relevance of each topic based on a preset list of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining finance/economy related words\n",
    "finance = [\"inflation\", \"stagflation\", \"interest rates\", \"recession\", \"volatility\", \"debt\", \"financial stress\", \"retirement\", \"investment\", \"uncertainity\",\"auto\", \"saving\", \"credit\", \"bugettin\", \"bank account\", \"net worth\", \"loans\", \"growth\", \"exchange rate\", \"trade\", \"stock market\", \"walstreet\", \"balance of payments\", \"industry\", \"bank\", \"policy\", \"asset\", \"bond\", \"equity\", \"debt\", \"money\", \"currency\", \"economy\", \"finance\", \"financial\", \"market\", \"stock\", \"trade\", \"business\", \"economic\", \"banking\", \"capital\", \"capitalism\", \"capitalist\", \"outlook\", \"forecast\", \"prediction\", \"prediction\", \"forecast\", \"outlook\", \"forecast\", \n",
    "          \"pr\", \"public relations\", \"public\", \"relief\", \"real estate\", \"housing\", \"mortgage\", \"home\", \"crisis\", \"crash\", \"bubble\", \"bust\", \"recession\", \"depression\", \"recovery\", \"finance\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):\n",
    "    from collections import Counter\n",
    "    from math import sqrt\n",
    "\n",
    "    # count the characters in word\n",
    "    cw = Counter(word)\n",
    "    # precomputes a set of the different characters\n",
    "    sw = set(cw)\n",
    "    # precomputes the \"length\" of the word vector\n",
    "    lw = sqrt(sum(c*c for c in cw.values()))\n",
    "\n",
    "    # return a tuple\n",
    "    return cw, sw, lw\n",
    "\n",
    "def cosdis(v1, v2):\n",
    "    # which characters are common to the two words?\n",
    "    common = v1[1].intersection(v2[1])\n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]\n",
    "\n",
    "\n",
    "def find_relevance(word, threshold = 0.0):\n",
    "    total = []\n",
    "    for j in word:\n",
    "        relevance = []\n",
    "        for i in finance:\n",
    "            relevance.append(cosdis(word2vec(word), word2vec(i)))\n",
    "        total.append(max(relevance))\n",
    "    return max(total)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try_ = top5_articlesday.groupby(\"date\", as_index = False).agg({'cleaned_topic': list})\n",
    "\n",
    "try_[\"relevance\"] = try_[\"cleaned_topic\"].apply(lambda x: find_relevance(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [hong kong stock, worldwide, business finance ...\n",
       "1       [tech center, canadian stock, work week, tech ...\n",
       "2       [business finance asia, business finance asia,...\n",
       "3       [tech brief, canadian stock, major business ne...\n",
       "4       [tech center, america, washington wire, tech b...\n",
       "                              ...                        \n",
       "8036    [world, review outlook, review outlook, review...\n",
       "8037    [review outlook, review outlook, review outloo...\n",
       "8038    [review outlook, review outlook, review outloo...\n",
       "8039    [review outlook, review outlook, asia, review ...\n",
       "8040    [review outlook, review outlook, politics, rev...\n",
       "Name: cleaned_topic, Length: 8041, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_[\"cleaned_topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\rahul\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp39-cp39-win_amd64.whl (172.3 MB)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.15.1-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from torchtext) (4.64.0)\n",
      "Collecting torchdata==0.6.0\n",
      "  Downloading torchdata-0.6.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (1.22.4)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (2.0.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch==2.0.0->torchtext) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from torch==2.0.0->torchtext) (4.5.0)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch==2.0.0->torchtext) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch==2.0.0->torchtext) (2.11.3)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch==2.0.0->torchtext) (3.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in d:\\anaconda\\lib\\site-packages (from torchdata==0.6.0->torchtext) (1.26.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch==2.0.0->torchtext) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Installing collected packages: torchdata, torchtext\n",
      "Successfully installed torchdata-0.6.0 torchtext-0.15.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch\n",
    "! pip install torchtext\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [08:39, 1.66MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:59<00:00, 6683.28it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_finance(word):\n",
    "    average = [] #this is the average for a given data\n",
    "    for i in range(len(word)):\n",
    "        if type(word[i]) == str:\n",
    "            try_i = word[i] #redefine word to split\n",
    "            try_i = try_i.split(\" \") # split the word\n",
    "            if len(try_i) > 1: #check if word can be split\n",
    "                average_1 = []\n",
    "                for j in try_i: #for each word in finance\n",
    "                    average_2  = []\n",
    "                    for k in finance:#for each word in the split word\n",
    "                        try:\n",
    "                            average_2.append(torch.cosine_similarity(glove[k].unsqueeze(0), glove[j].unsqueeze(0)).item()) #find the cosine similarity\n",
    "                        except:\n",
    "                            pass\n",
    "                    average_1.append(max(average_2)) #find the max cosine similarity\n",
    "            else:\n",
    "                average_1 = []\n",
    "                for j in finance:\n",
    "                    try:\n",
    "                        average_1.append(torch.cosine_similarity(glove[j].unsqueeze(0), glove[try_i[0]].unsqueeze(0)).item())\n",
    "                    except:\n",
    "                        pass\n",
    "            average.append(max(average_1))\n",
    "        else:\n",
    "            average.append(0)\n",
    "    return sum(average)/len(average)\n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_[\"relevance\"] = try_[\"cleaned_topic\"].apply(lambda x: cosine_similarity_finance(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.927433\n",
       "1       0.763274\n",
       "2       0.926602\n",
       "3       0.814249\n",
       "4       0.752776\n",
       "          ...   \n",
       "8036    0.874832\n",
       "8037    0.749614\n",
       "8038    1.000000\n",
       "8039    0.947215\n",
       "8040    0.941916\n",
       "Name: relevance, Length: 8041, dtype: float64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday = pd.merge(top5_articlesday, try_, on = \"date\", how = \"left\")\n",
    "grouped = top5_articlesday.groupby(\"date\", as_index = False).agg({\"headline_sentiment\": \"mean\", \"paragraph_sentiment\": \"mean\", \"relevance\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_articlesday.to_csv(\"top5_articlesday.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
